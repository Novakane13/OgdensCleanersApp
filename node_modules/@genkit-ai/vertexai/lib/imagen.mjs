import {
  __async,
  __spreadProps,
  __spreadValues
} from "./chunk-WFI2LP4G.mjs";
import {
  defineModel,
  GenerationCommonConfigSchema,
  getBasicUsageStats,
  modelRef
} from "@genkit-ai/ai/model";
import z from "zod";
import { predictModel } from "./predict.js";
const ImagenConfigSchema = GenerationCommonConfigSchema.extend({
  /** Language of the prompt text. */
  language: z.enum(["auto", "en", "es", "hi", "ja", "ko", "pt", "zh-TW", "zh", "zh-CN"]).optional(),
  /** Desired aspect ratio of output image. */
  aspectRatio: z.enum(["1:1", "9:16", "16:9", "3:4", "4:3"]).optional(),
  /**
   * A negative prompt to help generate the images. For example: "animals"
   * (removes animals), "blurry" (makes the image clearer), "text" (removes
   * text), or "cropped" (removes cropped images).
   **/
  negativePrompt: z.string().optional(),
  /**
   * Any non-negative integer you provide to make output images deterministic.
   * Providing the same seed number always results in the same output images.
   * Accepted integer values: 1 - 2147483647.
   **/
  seed: z.number().optional(),
  /** Your GCP project's region. e.g.) us-central1, europe-west2, etc. **/
  location: z.string().optional(),
  /** Allow generation of people by the model. */
  personGeneration: z.enum(["dont_allow", "allow_adult", "allow_all"]).optional(),
  /** Adds a filter level to safety filtering. */
  safetySetting: z.enum(["block_most", "block_some", "block_few", "block_fewest"]).optional(),
  /** Add an invisible watermark to the generated images. */
  addWatermark: z.boolean().optional(),
  /** Cloud Storage URI to store the generated images. **/
  storageUri: z.string().optional(),
  /** Mode must be set for upscaling requests. */
  mode: z.enum(["upscale"]).optional(),
  /**
   * Describes the editing intention for the request.
   *
   * Refer to https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/imagen-api#edit_images_2 for details.
   */
  editConfig: z.object({
    /** Describes the editing intention for the request. */
    editMode: z.enum([
      "inpainting-insert",
      "inpainting-remove",
      "outpainting",
      "product-image"
    ]).optional(),
    /** Prompts the model to generate a mask instead of you needing to provide one. Consequently, when you provide this parameter you can omit a mask object. */
    maskMode: z.object({
      maskType: z.enum(["background", "foreground", "semantic"]),
      classes: z.array(z.number()).optional()
    }).optional(),
    maskDilation: z.number().optional(),
    guidanceScale: z.number().optional(),
    productPosition: z.enum(["reposition", "fixed"]).optional()
  }).passthrough().optional(),
  /** Upscale config object. */
  upscaleConfig: z.object({ upscaleFactor: z.enum(["x2", "x4"]) }).optional()
}).passthrough();
const imagen2 = modelRef({
  name: "vertexai/imagen2",
  info: {
    label: "Vertex AI - Imagen2",
    versions: ["imagegeneration@006", "imagegeneration@005"],
    supports: {
      media: false,
      multiturn: false,
      tools: false,
      systemRole: false,
      output: ["media"]
    }
  },
  version: "imagegeneration@006",
  configSchema: ImagenConfigSchema
});
const imagen3 = modelRef({
  name: "vertexai/imagen3",
  info: {
    label: "Vertex AI - Imagen3",
    versions: ["imagen-3.0-generate-001"],
    supports: {
      media: true,
      multiturn: false,
      tools: false,
      systemRole: false,
      output: ["media"]
    }
  },
  version: "imagen-3.0-generate-001",
  configSchema: ImagenConfigSchema
});
const imagen3Fast = modelRef({
  name: "vertexai/imagen3-fast",
  info: {
    label: "Vertex AI - Imagen3 Fast",
    versions: ["imagen-3.0-fast-generate-001"],
    supports: {
      media: false,
      multiturn: false,
      tools: false,
      systemRole: false,
      output: ["media"]
    }
  },
  version: "imagen-3.0-fast-generate-001",
  configSchema: ImagenConfigSchema
});
const SUPPORTED_IMAGEN_MODELS = {
  imagen2,
  imagen3,
  "imagen3-fast": imagen3Fast
};
function extractText(request) {
  return request.messages.at(-1).content.map((c) => c.text || "").join("");
}
function toParameters(request) {
  var _a;
  const out = __spreadValues({
    sampleCount: (_a = request.candidates) != null ? _a : 1
  }, request == null ? void 0 : request.config);
  for (const k in out) {
    if (!out[k])
      delete out[k];
  }
  return out;
}
function extractMaskImage(request) {
  var _a, _b, _c;
  return (_c = (_b = (_a = request.messages.at(-1)) == null ? void 0 : _a.content.find((p) => {
    var _a2;
    return !!p.media && ((_a2 = p.metadata) == null ? void 0 : _a2.type) === "mask";
  })) == null ? void 0 : _b.media) == null ? void 0 : _c.url.split(",")[1];
}
function extractBaseImage(request) {
  var _a, _b, _c;
  return (_c = (_b = (_a = request.messages.at(-1)) == null ? void 0 : _a.content.find(
    (p) => {
      var _a2, _b2;
      return !!p.media && (!((_a2 = p.metadata) == null ? void 0 : _a2.type) || ((_b2 = p.metadata) == null ? void 0 : _b2.type) === "base");
    }
  )) == null ? void 0 : _b.media) == null ? void 0 : _c.url.split(",")[1];
}
function imagenModel(name, client, options) {
  const modelName = `vertexai/${name}`;
  const model = SUPPORTED_IMAGEN_MODELS[name];
  if (!model)
    throw new Error(`Unsupported model: ${name}`);
  const predictClients = {};
  const predictClientFactory = (request) => {
    var _a, _b;
    const requestLocation = ((_a = request.config) == null ? void 0 : _a.location) || options.location;
    if (!predictClients[requestLocation]) {
      predictClients[requestLocation] = predictModel(
        client,
        __spreadProps(__spreadValues({}, options), {
          location: requestLocation
        }),
        ((_b = request.config) == null ? void 0 : _b.version) || model.version || name
      );
    }
    return predictClients[requestLocation];
  };
  return defineModel(
    __spreadProps(__spreadValues({
      name: modelName
    }, model.info), {
      configSchema: ImagenConfigSchema
    }),
    (request) => __async(this, null, function* () {
      const instance = {
        prompt: extractText(request)
      };
      const baseImage = extractBaseImage(request);
      if (baseImage) {
        instance.image = { bytesBase64Encoded: baseImage };
      }
      const maskImage = extractMaskImage(request);
      if (maskImage) {
        instance.mask = {
          image: { bytesBase64Encoded: maskImage }
        };
      }
      const req = {
        instances: [instance],
        parameters: toParameters(request)
      };
      const predictClient = predictClientFactory(request);
      const response = yield predictClient([instance], toParameters(request));
      const candidates = response.predictions.map((p, i) => {
        const b64data = p.bytesBase64Encoded;
        const mimeType = p.mimeType;
        return {
          index: i,
          finishReason: "stop",
          message: {
            role: "model",
            content: [
              {
                media: {
                  url: `data:${mimeType};base64,${b64data}`,
                  contentType: mimeType
                }
              }
            ]
          }
        };
      });
      return {
        candidates,
        usage: __spreadProps(__spreadValues({}, getBasicUsageStats(request.messages, candidates)), {
          custom: { generations: candidates.length }
        }),
        custom: response
      };
    })
  );
}
export {
  SUPPORTED_IMAGEN_MODELS,
  imagen2,
  imagen3,
  imagen3Fast,
  imagenModel
};
//# sourceMappingURL=imagen.mjs.map